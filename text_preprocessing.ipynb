{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"<p>A boy name Aryan  walk into jungle of raw text üòµ‚Äçüí´ which were full of messy datas.</p> He triped <b>over</b> <a href=\"#\">HTML tags</a> and dodge flying üòÜ emojis and get tangle in <i>URLs</i>, user@mentions and #hashtags!!! The air was very thick,,, with punctuations???? and contraction like <u>don't</u>, grammatical error and <strong>  </strong>. SOME words SHOUTED IN UPPERCASE!! while otherz were mispelled or repeated. <div>Using tools he was cleaned the jungle well!</div>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>a boy name aryan  walk into jungle of raw text üòµ‚Äçüí´ which were full of messy datas.</p> he triped <b>over</b> <a href=\"#\">html tags</a> and dodge flying üòÜ emojis and get tangle in <i>urls</i>, user@mentions and #hashtags!!! the air was very thick,,, with punctuations???? and contraction like <u>don't</u>, grammatical error and <strong>  </strong>. some words shouted in uppercase!! while otherz were mispelled or repeated. <div>using tools he was cleaned the jungle well!</div>\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a boy name aryan  walk into jungle of raw text üòµ‚Äçüí´ which were full of messy datas. he triped over  and dodge flying üòÜ emojis and get tangle in , user@mentions and #hashtags!!! the air was very thick,,, with punctuations???? and contraction like , grammatical error and   . some words shouted in uppercase!! while otherz were mispelled or repeated. using tools he was cleaned the jungle well!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# removing a i u tags where text is bet tags where text is not imp so <a>ur;</a>\n",
    "def remove_html_tags(text):\n",
    "    text = re.sub(r'<(i|u|a).*>.*?<\\/\\1>','',text)\n",
    "    text = re.sub(r'<.*?>','',text)\n",
    "    return text\n",
    "text = remove_html_tags(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out this awesome website: for more information. Also, visit our blog at to read the latest updates! \n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"Check out this awesome website: https://www.example.com for more information. Also, visit our blog at http://blog.example.org/articles to read the latest updates! \"\"\"\n",
    "def remove_urls(text):\n",
    "    data = re.sub(r'(https?|www):\\/\\/\\S.*?\\s','',text)\n",
    "    return data\n",
    "\n",
    "data = remove_urls(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a boy name aryan  walk into jungle of raw text üòµ‚Äçüí´ which were full of messy datas he triped over  and dodge flying üòÜ emojis and get tangle in  usermentions and hashtags the air was very thick with punctuations and contraction like  grammatical error and    some words shouted in uppercase while otherz were mispelled or repeated using tools he was cleaned the jungle well\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punchuation = string.punctuation\n",
    "def remove_punchuation(text):\n",
    "    #it removes all punchutaions in the text\n",
    "    return text.translate(str.maketrans('','',punchuation))\n",
    "\n",
    "text = remove_punchuation(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle with care\n",
    "from chat_words import chat_words_dict\n",
    "def chat_word_treatment(data):\n",
    "    words = data.split()\n",
    "    new_text = []\n",
    "    for word in words:\n",
    "        if word in chat_words_dict.keys():\n",
    "            new_text.append(chat_words_dict[word])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am -d laugher away from keyboard\n"
     ]
    }
   ],
   "source": [
    "print(chat_word_treatment('i am sic af'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a boy name bryan  walk into jungle of raw text üòµ‚Äçüí´ which were full of mess data he tried over  and dodge flying üòÜ femoris and get tangle in  usermentions and hashtags the air was very thick with punctuation and contraction like  grammatical error and    some words shouted in uppercase while other were dispelled or repeated using tools he was cleaned the jungle well\n"
     ]
    }
   ],
   "source": [
    "#using Textblob\n",
    "# try also ssymspellpy\n",
    "from textblob import TextBlob\n",
    "def correct_spell(text):\n",
    "    txtblob = TextBlob(text)\n",
    "    return txtblob.correct().string\n",
    "text = correct_spell(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' boy name bryan walk  jungle  raw text üòµ\\u200düí´   full  mess data  tried   dodge flying üòÜ femoris  get tangle  usermentions  hashtags  air   thick  punctuation  contraction like grammatical error   words shouted  uppercase    dispelled  repeated using tools   cleaned  jungle well'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\" \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "        u\"\\U00002500-\\U00002BEF\" \n",
    "        u\"\\U00002702-\\U000027B0\" \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\" \n",
    "        u\"\\u200d\"                \n",
    "        u\"\\u2640-\\u2642\"         \n",
    "        u\"\\u2600-\\u2B55\"         \n",
    "        u\"\\u23cf\"                \n",
    "        u\"\\u23e9\"                \n",
    "        u\"\\u231a\"                \n",
    "        u\"\\u3030\"                \n",
    "        u\"\\ufe0f\"                \n",
    "        u\"\\u2069\"                \n",
    "        u\"\\u2066\"                \n",
    "        u\"\\u200c\"                \n",
    "        u\"\\u2068\"                \n",
    "        u\"\\u2067\"                \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie. It was üòòüòò\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "# it should be done at begaing so the symbol : will be removed with punctuaition\n",
    "#handle with care if we want to use the emoji context so keep it other wise delete it\n",
    "import emoji\n",
    "print(emoji.demojize('Python is üî•'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'pune']\n"
     ]
    }
   ],
   "source": [
    "# word tokenize\n",
    "data = \" i love pune \"\n",
    "print(data.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love pune', ' misal pav from pune is my favourite dish']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenize\n",
    "data  = \"i love pune. misal pav from pune is my favourite dish\"\n",
    "print(data.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions can be also used for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\bhosa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK for tokenization\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Word tokenizer nltk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m word_tokens = word_tokenize(\u001b[43mtext\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(word_tokens)\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#Word tokenizer nltk\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The library was unusually quiet, with only the soft rustling of pages breaking the silence.', 'A cool breeze drifted in through the open window, carrying the scent of fresh rain.', 'Mark sat by the window, completely absorbed in the mystery novel he had just started.']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenizer\n",
    "text = \"The library was unusually quiet, with only the soft rustling of pages breaking the silence. A cool breeze drifted in through the open window, carrying the scent of fresh rain. Mark sat by the window, completely absorbed in the mystery novel he had just started.\"\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'P.hd', 'in', 'A.i']\n",
      "['we', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', '1900', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'i have P.hd in A.i'\n",
    "print(word_tokenize(sent1))\n",
    "sent2 = \"we're here to help! mail us at 1900@gmail.com\"\n",
    "#nltk also failed at a gmail.. and were\n",
    "print(word_tokenize(sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spacy is good in some cases than nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp  = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to doc this text before using spacy\n",
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "have\n",
      "P.hd\n",
      "in\n",
      "A.i\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "1900@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# here we're is not get as a single word\n",
    "# but gmail is as it is\n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "sample = \"walk walks walked walking\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the runner were run rapidli across the playground, chase their dream without hesitation. despit face difficulties, they kept move forward, motiv by their passion and dedication. the happi of complet the marathon wa beyond explanation. mani spectat were cheer loudly, appreci the remark effort made by the participants. some of them were smile joyfully, while other were captur the moment with their cameras. fli flag and wave banner fill the atmospher with celebration. everyon wa enjoy themselv immensely, forget their daili struggl and responsibilities.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "sample = \"The runners were running rapidly across the playground, chasing their dreams without hesitation. Despite facing difficulties, they kept moving forward, motivated by their passion and dedication. The happiness of completing the marathon was beyond explanation. Many spectators were cheering loudly, appreciating the remarkable efforts made by the participants. Some of them were smiling joyfully, while others were capturing the moment with their cameras. Flying flags and waving banners filled the atmosphere with celebration. Everyone was enjoying themselves immensely, forgetting their daily struggles and responsibilities.\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhosa\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  : The\n",
      "runners  : runners\n",
      "were  : be\n",
      "running  : run\n",
      "rapidly  : rapidly\n",
      "across  : across\n",
      "the  : the\n",
      "playground  : playground\n",
      "chasing  : chase\n",
      "their  : their\n",
      "dreams  : dream\n",
      "without  : without\n",
      "hesitation  : hesitation\n",
      "Despite  : Despite\n",
      "facing  : face\n",
      "difficulties  : difficulties\n",
      "they  : they\n",
      "kept  : keep\n",
      "moving  : move\n",
      "forward  : forward\n",
      "motivated  : motivate\n",
      "by  : by\n",
      "their  : their\n",
      "passion  : passion\n",
      "and  : and\n",
      "dedication  : dedication\n",
      "The  : The\n",
      "happiness  : happiness\n",
      "of  : of\n",
      "completing  : complete\n",
      "the  : the\n",
      "marathon  : marathon\n",
      "was  : be\n",
      "beyond  : beyond\n",
      "explanation  : explanation\n",
      "Many  : Many\n",
      "spectators  : spectators\n",
      "were  : be\n",
      "cheering  : cheer\n",
      "loudly  : loudly\n",
      "appreciating  : appreciate\n",
      "the  : the\n",
      "remarkable  : remarkable\n",
      "efforts  : efforts\n",
      "made  : make\n",
      "by  : by\n",
      "the  : the\n",
      "participants  : participants\n",
      "Some  : Some\n",
      "of  : of\n",
      "them  : them\n",
      "were  : be\n",
      "smiling  : smile\n",
      "joyfully  : joyfully\n",
      "while  : while\n",
      "others  : others\n",
      "were  : be\n",
      "capturing  : capture\n",
      "the  : the\n",
      "moment  : moment\n",
      "with  : with\n",
      "their  : their\n",
      "cameras  : cameras\n",
      "Flying  : Flying\n",
      "flags  : flag\n",
      "and  : and\n",
      "waving  : wave\n",
      "banners  : banners\n",
      "filled  : fill\n",
      "the  : the\n",
      "atmosphere  : atmosphere\n",
      "with  : with\n",
      "celebration  : celebration\n",
      "Everyone  : Everyone\n",
      "was  : be\n",
      "enjoying  : enjoy\n",
      "themselves  : themselves\n",
      "immensely  : immensely\n",
      "forgetting  : forget\n",
      "their  : their\n",
      "daily  : daily\n",
      "struggles  : struggle\n",
      "and  : and\n",
      "responsibilities  : responsibilities\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "sample = \"The runners were running rapidly across the playground, chasing their dreams without hesitation. Despite facing difficulties, they kept moving forward, motivated by their passion and dedication. The happiness of completing the marathon was beyond explanation. Many spectators were cheering loudly, appreciating the remarkable efforts made by the participants. Some of them were smiling joyfully, while others were capturing the moment with their cameras. Flying flags and waving banners filled the atmosphere with celebration. Everyone was enjoying themselves immensely, forgetting their daily struggles and responsibilities.\"\n",
    "sample = remove_punchuation(sample)\n",
    "words = word_tokenize(sample)\n",
    "for word in words:\n",
    "    print(f\"{word}  : {word_lemmatizer.lemmatize(word,'v')}\")\n",
    "# use u and n also separate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
